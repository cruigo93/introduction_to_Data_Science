{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Feature Engineering and Feature Selection</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.067528Z",
     "start_time": "2018-03-15T14:06:02.181930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Let's load the dataset from Renthop right away\n",
    "with open('train.json', 'r') as raw_data:\n",
    "    data = json.load(raw_data)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article outline\n",
    "\n",
    "1. Feature Extraction\n",
    "        1. Texts\n",
    "        2. Images\n",
    "        3. Geospatial data\n",
    "        4. Date and time\n",
    "        5. Time series, web, etc.\n",
    "\n",
    "2. Feature transformations\n",
    "        1. Normalization and changing distribution\n",
    "        2. Interactions\n",
    "        3. Filling in the missing values\n",
    "\n",
    "3. Feature selection\n",
    "        1. Statistical approaches\n",
    "        2. Selection by modeling\n",
    "        3. Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In practice, data rarely comes in the form of ready-to-use matrices. That's why every task begins with feature extraction. Sometimes, it can be enough to read the csv file and convert it into `numpy.array`, but this is a rare exception. Let's look at some of the popular types of data from which features can be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts\n",
    "\n",
    "Text is a type of data that can come in different formats; there are so many text processing methods that cannot fit in a single article. Nevertheless, we will review the most popular ones.\n",
    "\n",
    "Before working with text, one must tokenize it. Tokenization implies splitting the text into units (hence, tokens). Most simply, tokens are just the words. But splitting by word can lose some of the meaning -- \"Santa Barbara\" is one token, not two, but \"rock'n'roll\" should not be split into two tokens. There are ready-to-use tokenizers that take into account peculiarities of the language, but they make mistakes as well, especially when you work with specific sources of text (newspapers, slang, misspellings, typos).\n",
    "\n",
    "After tokenization, you will normalize the data. For text, this is about stemming and/or lemmatization; these are similar processes used to process different forms of a word. One can read about the difference between them [here](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).\n",
    "\n",
    "So, now that we have turned the document into a sequence of words, we can represent it with vectors. The easiest approach is called Bag of Words: we create a vector with the length of the dictionary, compute the number of occurrences of each word in the text, and place that number of occurrences in the appropriate position in the vector. The process described looks simpler in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.087385Z",
     "start_time": "2018-03-15T14:06:07.068964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0. 1. 0. 1.]\n",
      "[1. 0. 0. 0. 1. 1. 1.]\n",
      "[1. 1. 1. 2. 2. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce \n",
    "import numpy as np\n",
    "\n",
    "texts = [['i', 'have', 'a', 'cat'], \n",
    "        ['he', 'have', 'a', 'dog'], \n",
    "        ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog']]\n",
    "\n",
    "dictionary = list(enumerate(set(list(reduce(lambda x, y: x + y, texts)))))\n",
    "\n",
    "def vectorize(text): \n",
    "    vector = np.zeros(len(dictionary)) \n",
    "    for i, word in dictionary: \n",
    "        num = 0 \n",
    "        for w in text: \n",
    "            if w == word: \n",
    "                num += 1 \n",
    "        if num: \n",
    "            vector[i] = num \n",
    "    return vector\n",
    "\n",
    "for t in texts: \n",
    "    print(vectorize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an illustration of the process:\n",
    "\n",
    "![image](img/bag_of_words.png)\n",
    "\n",
    "This is an extremely naive implementation. In practice, you need to consider stop words, the maximum length of the dictionary, more efficient data structures (usually text data is converted to a sparse vector), etc.\n",
    "\n",
    "When using algorithms like Bag of Words, we lose the order of the words in the text, which means that the texts \"i have no cows\" and \"no, i have cows\" will appear identical after vectorization when, in fact, they have the opposite meaning. To avoid this problem, we can revisit our tokenization step and use N-grams (the *sequence* of N consecutive tokens) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:08.998673Z",
     "start_time": "2018-03-15T14:06:07.088376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.002804Z",
     "start_time": "2018-03-15T14:06:08.999801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cows': 0, 'have': 1, 'no': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.110448Z",
     "start_time": "2018-03-15T14:06:09.003924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.218867Z",
     "start_time": "2018-03-15T14:06:09.113204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cows': 0,\n",
       " 'have': 1,\n",
       " 'have cows': 2,\n",
       " 'have no': 3,\n",
       " 'no': 4,\n",
       " 'no cows': 5,\n",
       " 'no have': 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:13:25.767656Z",
     "start_time": "2018-03-14T14:13:25.763924Z"
    }
   },
   "source": [
    "Also note that one does not have to use only words. In some cases, it is possible to generate N-grams of characters. This approach would be able to account for similarity of related words or handle typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.774148Z",
     "start_time": "2018-03-15T14:06:09.220060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8284271247461903, 3.1622776601683795, 3.3166247903554)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3,3), analyzer='char_wb')\n",
    "\n",
    "n1, n2, n3, n4 = vect.fit_transform(['andersen', 'petersen', 'petrov', 'smith']).toarray()\n",
    "\n",
    "euclidean(n1, n2), euclidean(n2, n3), euclidean(n3, n4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' an': 0,\n",
       " ' pe': 1,\n",
       " ' sm': 2,\n",
       " 'and': 3,\n",
       " 'der': 4,\n",
       " 'en ': 5,\n",
       " 'ers': 6,\n",
       " 'ete': 7,\n",
       " 'etr': 8,\n",
       " 'ith': 9,\n",
       " 'mit': 10,\n",
       " 'nde': 11,\n",
       " 'ov ': 12,\n",
       " 'pet': 13,\n",
       " 'rov': 14,\n",
       " 'rse': 15,\n",
       " 'sen': 16,\n",
       " 'smi': 17,\n",
       " 'ter': 18,\n",
       " 'th ': 19,\n",
       " 'tro': 20}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding onto the Bag of Words idea: words that are rarely found in the corpus (in all the documents of this dataset) but are present in this particular document might be more important. Then it makes sense to increase the weight of more domain-specific words to separate them out from common words. This approach is called TF-IDF (term frequency-inverse document frequency), which cannot be written in a few lines, so you should look into the details in references such as [this wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The default option is as follows:\n",
    "\n",
    "$$ \\large idf(t,D) = \\log\\frac{\\mid D\\mid}{df(d,t)+1} $$\n",
    "\n",
    "$$ \\large tfidf(t,d,D) = tf(t,d) \\times idf(t,D) $$\n",
    "\n",
    "Analogs of Bag of Words can be found outside of text problems e.g. bag of sites in the [Catch Me If You Can competition](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking), [bag of apps](https://www.kaggle.com/xiaoml/talkingdata-mobile-user-demographics/bag-of-app-id-python-2-27392), [bag of events](http://www.interdigital.com/download/58540a46e3b9659c9f000372), etc.\n",
    "\n",
    "![image](img/bag_of_words.png)\n",
    "\n",
    "Using these algorithms, it is possible to obtain a working solution for a simple problem, which can serve as a baseline. However, for those who do not like the classics, there are new approaches. The most popular method in the new wave is Word2Vec, but there are a few alternatives as well (GloVe, Fasttext, etc.).\n",
    "\n",
    "Word2Vec is a special case of the word embedding algorithms. Using Word2Vec and similar models, we can not only vectorize words in a high-dimensional space (typically a few hundred dimensions) but also compare their semantic similarity. This is a classic example of operations that can be performed on vectorized concepts: king - man + woman = queen.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/800/1*K5X4N-MJKt8FGFtrTHwidg.gif)\n",
    "\n",
    "It is worth noting that this model does not comprehend the meaning of the words but simply tries to position the vectors such that words used in common context are close to each other. If this is not taken into account, a lot of fun examples will come up.\n",
    "\n",
    "Such models need to be trained on very large datasets in order for the vector coordinates to capture the semantics. A pretrained model for your own tasks can be downloaded [here](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models).\n",
    "\n",
    "Similar methods are applied in other areas such as bioinformatics. An unexpected application is [food2vec](https://jaan.io/food2vec-augmented-cooking-machine-intelligence/). You can probably think of a few other fresh ideas; the concept is universal enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial data\n",
    "\n",
    "Geographic data is not so often found in problems, but it is still useful to master the basic techniques for working with it, especially since there are quite a number of ready-to-use solutions in this field.\n",
    "\n",
    "Geospatial data is often presented in the form of addresses or coordinates of (Latitude, Longitude). Depending on the task, you may need two mutually-inverse operations: geocoding (recovering a point from an address) and reverse geocoding (recovering an address from a point). Both operations are accessible in practice via external APIs from Google Maps or OpenStreetMap. Different geocoders have their own characteristics, and the quality varies from region to region. Fortunately, there are universal libraries like [geopy](https://github.com/geopy/geopy) that act as wrappers for these external services.\n",
    "\n",
    "If you have a lot of data, you will quickly reach the limits of external API. Besides, it is not always the fastest to receive information via HTTP. Therefore, it is necessary to consider using a local version of OpenStreetMap.\n",
    "\n",
    "If you have a small amount of data, enough time, and no desire to extract fancy features, you can use `reverse_geocoder` in lieu of OpenStreetMap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T15:12:50.468269Z",
     "start_time": "2018-03-14T15:12:50.455393Z"
    }
   },
   "source": [
    "```python\n",
    "import reverse_geocoder as revgc\n",
    "\n",
    "revgc.search((df.latitude, df.longitude))\n",
    "Loading formatted geocoded file... \n",
    "\n",
    "Out: [OrderedDict([('lat', '40.74482'), \n",
    "                   ('lon', '-73.94875'), \n",
    "                   ('name', 'Long Island City'), \n",
    "                   ('admin1', 'New York'), \n",
    "                   ('admin2', 'Queens County'), \n",
    "                   ('cc', 'US')])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with geoÑoding, we must not forget that addresses may contain typos, which makes the data cleaning step necessary. Coordinates contain fewer misprints, but its position can be incorrect due to GPS noise or bad accuracy in places like tunnels, downtown areas, etc. If the data source is a mobile device, the geolocation may not be determined by GPS but by WiFi networks in the area, which leads to holes in space and teleportation. While traveling along in Manhattan, there can suddenly be a WiFi location from Chicago.\n",
    "\n",
    "> WiFi location tracking is based on the combination of SSID and MAC-addresses, which may correspond to different points e.g. federal provider standardizes the firmware of routers up to MAC-address and places them in different cities. Even a company's move to another office with its routers can cause issues.\n",
    "\n",
    "The point is usually located among infrastructure. Here, you can really unleash your imagination and invent features based on your life experience and domain knowledge: the proximity of a point to the subway, the number of stories in the building, the distance to the nearest store, the number of ATMs around, etc. For any task, you can easily come up with dozens of features and extract them from various external sources. For problems outside an urban environment, you may consider features from more specific sources e.g. the height above sea level.\n",
    "\n",
    "If two or more points are interconnected, it may be worthwhile to extract features from the route between them. In that case, distances (great circle distance and road distance calculated by the routing graph), number of turns with the ratio of left to right turns, number of traffic lights, junctions, and bridges will be useful. In one of my own tasks, I generated a feature called \"the complexity of the road\", which computed the graph-calculated distance divided by the GCD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and time\n",
    "\n",
    "You would think that date and time are standardized because of their prevalence, but, nevertheless, some pitfalls remain.\n",
    "\n",
    "Let's start with the day of the week, which are easy to turn into 7 dummy variables using one-hot encoding. In addition, we will also create a separate binary feature for the weekend called `is_weekend`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df['dow'] = df['created'].apply(lambda x: x.date().weekday())\n",
    "df['is_weekend'] = df['created'].apply(lambda x: 1 if x.date().weekday() in (5, 6) else 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tasks may require additional calendar features. For example, cash withdrawals can be linked to a pay day; the purchase of a metro card, to the beginning of the month. In general, when working with time series data, it is a good idea to have a calendar with public holidays, abnormal weather conditions, and other important events.\n",
    "\n",
    "> Q: What do Chinese New Year, the New York marathon, and the Trump inauguration have in common?\n",
    "\n",
    "> A: They all need to be put on the calendar of potential anomalies.\n",
    "\n",
    "Dealing with hour (minute, day of the month ...) is not as simple as it seems. If you use the hour as a real variable, we slightly contradict the nature of data: `0<23` while `0:00:00 02.01> 01.01 23:00:00`. For some problems, this can be critical. At the same time, if you encode them as categorical variables, you'll breed a large numbers of features and lose information about proximity -- the difference between 22 and 23 will be the same as the difference between 22 and 7.\n",
    "\n",
    "There also exist some more esoteric approaches to such data like projecting the time onto a circle and using the two coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.782320Z",
     "start_time": "2018-03-15T14:06:27.772449Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_harmonic_features(value, period=24):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value), np.sin(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation preserves the distance between points, which is important for algorithms that estimate distance (kNN, SVM, k-means ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.883311Z",
     "start_time": "2018-03-15T14:06:27.784833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176380902050424"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "euclidean(make_harmonic_features(23), make_harmonic_features(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:28.250852Z",
     "start_time": "2018-03-15T14:06:27.884753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176380902050414"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(make_harmonic_features(9), make_harmonic_features(11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:28.801865Z",
     "start_time": "2018-03-15T14:06:28.252109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(make_harmonic_features(9), make_harmonic_features(21))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
